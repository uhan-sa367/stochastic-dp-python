{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_cell": true
   },
   "source": [
    "**SA367 &#x25aa; Mathematical Models for Decision Making &#x25aa; Spring 2022 &#x25aa; Uhan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_cell": true
   },
   "source": [
    "# Lesson 14. Solving stochastic dynamic programs with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's solve the stochastic dynamic program we formulated for the investment problem in Lesson 13\n",
    "\n",
    "\n",
    "* In this class, we will use a package called `stochasticdp` to set up and solve stochastic dynamic programs\n",
    "    - _Warning._ This is a package that I wrote. There may be some bugs.\n",
    "    - _Note._ This package is publicly available. Please feel free to use it in the future for other things. The source code is on [GitHub](https://github.com/nelsonuhan/stochasticdp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a stochastic dynamic program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use `stochasticdp`, we must first import it. In `stochasticdp`, we only need the object `StochasticDP`, so we can perform our import like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stochasticdp import StochasticDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall the investment problem from Lesson 13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem.__ Suppose you have \\\\$5,000 to invest. Over the next 3 years, you want to double your money.\n",
    "At the beginning of each of the next 3 years, you have an opportunity to invest in one of two investments: A or B. Both investments have uncertain profits. For an investment of \\\\$5,000, the profits are as follows:\n",
    "\n",
    "\n",
    "| Investment | Profit (\\$) | Probability |\n",
    "|:-----------|------------:|------------:|\n",
    "| A          | -5,000      | 0.3         |\n",
    "|            | 5,000       | 0.7         |\n",
    "| B          | 0           | 0.9         |\n",
    "|            | 5,000       | 0.1         |\n",
    "                                     \n",
    "You are allowed to make at most one investment each year, and can invest only \\\\$5,000 each time. Any additional money accumulated is left idle. Once you've accumulated \\\\$10,000, you stop investing.\n",
    "\n",
    "Formulate a stochastic dynamic program to find an investment policy that maximizes the probability you will have \\$10,000 after 3 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's walk through setting up the stochastic DP we formulated for this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We had defined 4 stages\n",
    "\n",
    "\n",
    "* To make things easier, let's renumber the stages so they start at $t = 0$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{stage } t = 0, 1, 2 & \\quad\\leftrightarrow\\quad \\text{beginning of year $t$}\\\\\n",
    "t = 3 & \\quad\\leftrightarrow\\quad \\text{end of process}\n",
    "\\end{aligned}\n",
    "$$\n",
    "    \n",
    "\n",
    "* In each stage, we defined 3 states:\n",
    "\n",
    "$$\n",
    "\\text{state } n \\in \\{0, 5000, 10000\\} \\quad\\leftrightarrow\\quad \\text{$n$ dollars in account}\n",
    "$$\n",
    "\n",
    "\n",
    "* At each stage and state, we defined 3 possible decisions:\n",
    "\n",
    "$$\n",
    "\\text{decision } x_t \\in \\{ \\text{A}, \\text{B}, \\text{no investment} \\}\n",
    "$$\n",
    "\n",
    "\n",
    "* The set of _allowable_ decisions changed, depending on the stage and state &mdash; we'll address this later\n",
    "\n",
    "\n",
    "* For now, we can initialize a stochastic dynamic program with these stages, states, and decisions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Number of stages\n",
    "number_of_stages = 4\n",
    "\n",
    "# List of states\n",
    "states = [0, 5000, 10000]\n",
    "\n",
    "# List of decisions\n",
    "decisions = ['A', 'B', 'no investment']\n",
    "\n",
    "# Initialize stochastic dynamic program - we want to maximize, so minimize = False\n",
    "dp = StochasticDP(number_of_stages, states, decisions, minimize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code above initializes a stochastic dynamic program called `dp`\n",
    "\n",
    "* Next, we need to add every transition that occurs with positive probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities and contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, let's tackle transitions from the state $n = 5000$:\n",
    "\n",
    "<center><img src=\"img/5000.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can add a transition from state $n$ to state $m$ in stage $t$ under decision $x$ with probability $p(m \\,|\\, n, t, x)$ and contribution $c(m \\,|\\, n, t, x)$ as follows:\n",
    "\n",
    "    ```python\n",
    "    dp.add_transition(stage=t, from_state=n, decision=x, to_state=n, probability=p, contribution=c)\n",
    "    ```\n",
    "\n",
    "\n",
    "* Remember that the contributions for all transitions are 0 in this stochastic DP\n",
    "\n",
    "\n",
    "* So, we can input the transition probabilities and contributions from state $n = 5000$ in stages $t = 0, 1, 2$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Transition probabilities and contributions from state n = 5000\n",
    "for t in range(number_of_stages - 1):\n",
    "    # Investment A\n",
    "    dp.add_transition(stage=t, from_state=5000, decision='A', to_state=10000, probability=0.7, contribution=0)\n",
    "    dp.add_transition(stage=t, from_state=5000, decision='A', to_state=0, probability=0.3, contribution=0)\n",
    "\n",
    "    # Investment B\n",
    "    dp.add_transition(stage=t, from_state=5000, decision='B', to_state=10000, probability=0.1, contribution=0)\n",
    "    dp.add_transition(stage=t, from_state=5000, decision='B', to_state=5000, probability=0.9, contribution=0)\n",
    "\n",
    "    # No investment\n",
    "    dp.add_transition(stage=t, from_state=5000, decision='no investment', to_state=5000, probability=1, contribution = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Quick check._ What can the sum of the transition probabilities from any decision node equal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your notes here. Double-click to edit._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Solution._ The transition probabilities from any decision node must add up to either 0 or 1. They will add up to 1 if the decision is allowable at that stage/state; otherwise they will add up to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, let's tackle the transitions from state $n = 0$:\n",
    "\n",
    "<center><img src=\"img/0.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, we can input the transition probabilities and contributions from state $n = 0$ in stages $t = 0, 1, 2$ like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Transition probabilities and contributions from state n = 0\n",
    "for t in range(number_of_stages - 1):\n",
    "    # No investment\n",
    "    dp.add_transition(stage=t, from_state=0, decision='no investment', to_state=0, probability=1, contribution = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can tackle the transitions from state $n = 10000$ in an almost identical way:\n",
    "\n",
    "<center><img src=\"img/10000.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Transition probabilities and contributions from state n = 10000\n",
    "for t in range(number_of_stages - 1):\n",
    "    # No investment\n",
    "    dp.add_transition(stage=t, from_state=10000, decision='no investment', to_state=10000, probability=1, contribution = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, we need to define the boundary conditions:\n",
    "\n",
    "$$\n",
    "f_3(10000) = 1 \\qquad f_3(5000) = 0 \\qquad f_3(0) = 0\n",
    "$$\n",
    "\n",
    "* In particular, we need to specify the value-to-go function at the last stage (in our case, $t = 3$) for each state\n",
    "\n",
    "* We can add the boundary conditions for state $n$ like this:\n",
    "\n",
    "```python\n",
    "dp.add_boundary(state=0, value=0)\n",
    "```\n",
    "\n",
    "* So, let's add the boundary conditions for our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Boundary conditions\n",
    "dp.add_boundary(state=10000, value=1)\n",
    "dp.add_boundary(state=5000, value=0)\n",
    "dp.add_boundary(state=0, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the stochastic dynamic program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once the stochastic DP is setup, we can solve it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Solve the stochastic dynamic program\n",
    "value, policy = dp.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the method `.solve()` outputs two objects: `value` and `policy`\n",
    "\n",
    "\n",
    "* `value[t, n]` is the value-to-go function $f_t(n)$ at stage $t$ and state $n$\n",
    "\n",
    "\n",
    "* `policy[t, n]` is the optimal decision $x_t^*$ that attains the value-to-go function $f_t(n)$ at stage $t$ and state $n$\n",
    "\n",
    "* First, let's see what the value-to-go function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(stage: 0, state: 0): 0\n",
      " (stage: 0, state: 5000): 0.757\n",
      " (stage: 0, state: 10000): 1\n",
      " (stage: 1, state: 0): 0\n",
      " (stage: 1, state: 5000): 0.73\n",
      " (stage: 1, state: 10000): 1\n",
      " (stage: 2, state: 0): 0\n",
      " (stage: 2, state: 5000): 0.7\n",
      " (stage: 2, state: 10000): 1\n",
      " (stage: 3, state: 0): 0\n",
      " (stage: 3, state: 5000): 0\n",
      " (stage: 3, state: 10000): 1}\n"
     ]
    }
   ],
   "source": [
    "# Examine the value-to-go function\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, let's look at the corresponding policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(stage: 0, state: 0): {'no investment'}\n",
      " (stage: 0, state: 5000): {'B'}\n",
      " (stage: 0, state: 10000): {'no investment'}\n",
      " (stage: 1, state: 0): {'no investment'}\n",
      " (stage: 1, state: 5000): {'B'}\n",
      " (stage: 1, state: 10000): {'no investment'}\n",
      " (stage: 2, state: 0): {'no investment'}\n",
      " (stage: 2, state: 5000): {'A'}\n",
      " (stage: 2, state: 10000): {'no investment'}}\n"
     ]
    }
   ],
   "source": [
    "# Examine the policy\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem.__ The Hit-and-Miss Manufacturing Company has received an order to supply one item of a particular type. However, manufacturing this item is difficult, and the customer has specified such stringent quality requirements that the company may have to produce more than one item to obtain an item that is acceptable.\n",
    "                \n",
    "The company estimates that each item of this type will be acceptable with probability 1/2 and defective with probability 1/2. Each item costs \\\\$100 to produce, and excess items are worthless. In addition, a setup cost of \\\\$300 must be incurred whenever the production process is setup for this item. The company has time to make no more than 3 production runs, and at most 5 items can be produced in each run. If an acceptable item has not been obtained by the end of the third production run, the manufacturer is in breach of contract and must pay a penalty of \\\\$1600.\n",
    "                \n",
    "The objective is to determine how many items to produce in each production run in order to minimize the total expected cost.\n",
    "\n",
    "Solve the stochastic DP we formulated in Lesson 13 for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value-to-go function is:\n",
      "{(stage: 0, state: 0): 0\n",
      " (stage: 0, state: 1): 675.0\n",
      " (stage: 1, state: 0): 0\n",
      " (stage: 1, state: 1): 700.0\n",
      " (stage: 2, state: 0): 0\n",
      " (stage: 2, state: 1): 800.0\n",
      " (stage: 3, state: 0): 0\n",
      " (stage: 3, state: 1): 1600}\n",
      "\n",
      "The corresponding policy is:\n",
      "{(stage: 0, state: 0): {0}\n",
      " (stage: 0, state: 1): {2}\n",
      " (stage: 1, state: 0): {0}\n",
      " (stage: 1, state: 1): {2, 3}\n",
      " (stage: 2, state: 0): {0}\n",
      " (stage: 2, state: 1): {3, 4}}\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "# Number of stages\n",
    "number_of_stages = 4\n",
    "\n",
    "# List of states\n",
    "states = [0, 1]\n",
    "\n",
    "# List of decisions\n",
    "decisions = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize stochastic dynamic program\n",
    "dp = StochasticDP(number_of_stages, states, decisions, minimize=True)\n",
    "\n",
    "# Transition probabilities and contributions from state n = 0\n",
    "for t in range(number_of_stages - 1):\n",
    "    for x in decisions:\n",
    "        if x > 0:\n",
    "            K = 300\n",
    "        else:\n",
    "            K = 0\n",
    "            \n",
    "        dp.add_transition(stage=t, from_state=0, decision=x, to_state=0, probability=1, contribution=K + 100*x)\n",
    "        dp.add_transition(stage=t, from_state=0, decision=x, to_state=1, probability=0, contribution=K + 100*x)        \n",
    "\n",
    "# Transition probabilities and contributions from state n = 1\n",
    "for t in range(number_of_stages - 1):\n",
    "    for x in decisions:\n",
    "        if x > 0:\n",
    "            K = 300\n",
    "        else:\n",
    "            K = 0\n",
    "            \n",
    "        dp.add_transition(stage=t, from_state=1, decision=x, to_state=0, probability=1 - (1/2)**x, contribution=K + 100*x)\n",
    "        dp.add_transition(stage=t, from_state=1, decision=x, to_state=1, probability=(1/2)**x, contribution=K + 100*x)\n",
    "        \n",
    "# Boundary conditions\n",
    "dp.add_boundary(state=0, value=0)\n",
    "dp.add_boundary(state=1, value=1600)\n",
    "\n",
    "# Solve the stochastic dynamic program\n",
    "value, policy = dp.solve()\n",
    "\n",
    "# Examine value-to-go\n",
    "print(\"The value-to-go function is:\")\n",
    "print(value)\n",
    "\n",
    "# Examine policy\n",
    "print(\"\\nThe corresponding policy is:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (Baytheon, revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baytheon has received an order to supply 2 guided missiles. In order to meet stringent quality requirements, the company may have to manufacture more than one missile to obtain an missile that is acceptable. The company has time to make no more than 3 production runs, and at most 2 missiles can be produced in each run. The probability distribution of acceptable missiles in a given run depends on how many missiles are produced:\n",
    "\n",
    "| Number of missiles produced | Prob. of 0 acceptable missiles | Prob. of 1 acceptable missile | Prob. of 2 acceptable missiles |\n",
    "|-----------------------------|--------------------------------|-------------------------------|--------------------------------| \n",
    "| 0                           | 1                              | 0                             | 0                              |\n",
    "| 1                           | 1/3                            | 2/3                           | 0                              |\n",
    "| 2                           | 1/4                            | 1/2                           | 1/4                            |\n",
    "    \n",
    "\n",
    "Each missile costs \\\\$100,000 to produce, and excess missiles are worthless. In addition, a setup cost of \\\\$50,000 must be incurred whenever the production process is setup for this item. If 2 acceptable missiles have not been obtained by the end of the third production run, Baytheon is in breach of contract and must pay a penalty of \\\\$1,000,000. The objective is to determine how many missiles to produce in each production run in order to minimize the total expected cost.\n",
    "\n",
    "Once upon a time, for homework, you formulated this problem as a stochastic dynamic program. (Note that the penalty value has changed.)\n",
    "\n",
    "1. Solve your dynamic program using `stochasticdp`.\n",
    "2. Interpret the output of your stochastic dynamic program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-to-go function:\n",
      "{(stage: 0, state: 0): 0\n",
      " (stage: 0, state: 1): 253.7037037037037\n",
      " (stage: 0, state: 2): 590.9722222222222\n",
      " (stage: 1, state: 0): 0\n",
      " (stage: 1, state: 1): 311.1111111111111\n",
      " (stage: 1, state: 2): 741.6666666666666\n",
      " (stage: 2, state: 0): 0\n",
      " (stage: 2, state: 1): 483.3333333333333\n",
      " (stage: 2, state: 2): 1000\n",
      " (stage: 3, state: 0): 0\n",
      " (stage: 3, state: 1): 1000\n",
      " (stage: 3, state: 2): 1000}\n",
      "\n",
      "Corresponding policy:\n",
      "{(stage: 0, state: 0): {0}\n",
      " (stage: 0, state: 1): {1}\n",
      " (stage: 0, state: 2): {2}\n",
      " (stage: 1, state: 0): {0}\n",
      " (stage: 1, state: 1): {1}\n",
      " (stage: 1, state: 2): {2}\n",
      " (stage: 2, state: 0): {0}\n",
      " (stage: 2, state: 1): {1}\n",
      " (stage: 2, state: 2): {0, 2}}\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "# Number of stages\n",
    "number_of_stages = 4\n",
    "\n",
    "# List of states\n",
    "states = [0, 1, 2]\n",
    "\n",
    "# List of decisions\n",
    "decisions = [0, 1, 2]\n",
    "\n",
    "# Initialize stochastic dynamic program\n",
    "dp = StochasticDP(number_of_stages, states, decisions, minimize=True)\n",
    "\n",
    "# Transition probabilities and contributions from state n = 2\n",
    "for t in range(number_of_stages - 1):\n",
    "    # Produce 2\n",
    "    dp.add_transition(stage=t, from_state=2, decision=2, to_state=2, probability=1/4, contribution=50 + 100*2)\n",
    "    dp.add_transition(stage=t, from_state=2, decision=2, to_state=1, probability=1/2, contribution=50 + 100*2)\n",
    "    dp.add_transition(stage=t, from_state=2, decision=2, to_state=0, probability=1/4, contribution=50 + 100*2)\n",
    "    \n",
    "    # Produce 1\n",
    "    dp.add_transition(stage=t, from_state=2, decision=1, to_state=2, probability=1/3, contribution=50 + 100*1)\n",
    "    dp.add_transition(stage=t, from_state=2, decision=1, to_state=1, probability=2/3, contribution=50 + 100*1)\n",
    "    \n",
    "    # Produce 0\n",
    "    dp.add_transition(stage=t, from_state=2, decision=0, to_state=2, probability=1, contribution=0)\n",
    "    \n",
    "# Transition probabilities and contributions from state n = 1\n",
    "for t in range(number_of_stages - 1):\n",
    "    # Produce 2\n",
    "    dp.add_transition(stage=t, from_state=1, decision=2, to_state=1, probability=1/4, contribution=50 + 100*2)\n",
    "    dp.add_transition(stage=t, from_state=1, decision=2, to_state=0, probability=1/2 + 1/4, contribution=50 + 100*2)\n",
    "    \n",
    "    # Produce 1\n",
    "    dp.add_transition(stage=t, from_state=1, decision=1, to_state=1, probability=1/3, contribution=50 + 100*1)\n",
    "    dp.add_transition(stage=t, from_state=1, decision=1, to_state=0, probability=2/3, contribution=50 + 100*1)\n",
    "    \n",
    "    # Produce 0\n",
    "    dp.add_transition(stage=t, from_state=1, decision=0, to_state=1, probability=1, contribution=0)\n",
    "\n",
    "# Transition probabilities and contributions from state n = 0\n",
    "for t in range(number_of_stages - 1):\n",
    "    # Produce 2\n",
    "    dp.add_transition(stage=t, from_state=0, decision=2, to_state=0, probability=1, contribution=50 + 100*2)\n",
    "    \n",
    "    # Produce 1\n",
    "    dp.add_transition(stage=t, from_state=0, decision=1, to_state=0, probability=1, contribution=50 + 100*1)\n",
    "    \n",
    "    # Produce 0\n",
    "    dp.add_transition(stage=t, from_state=0, decision=0, to_state=0, probability=1, contribution=0)\n",
    "        \n",
    "# Boundary conditions\n",
    "dp.add_boundary(state=0, value=0)\n",
    "dp.add_boundary(state=1, value=1000)\n",
    "dp.add_boundary(state=2, value=1000)\n",
    "\n",
    "# Solve the stochastic dynamic program\n",
    "value, policy = dp.solve()\n",
    "\n",
    "# Examine value-to-go\n",
    "print(\"Value-to-go function:\")\n",
    "print(value)\n",
    "\n",
    "# Examine policy\n",
    "print(\"\\nCorresponding policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your notes here. Double-click to edit._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Solution._\n",
    "\n",
    "* The minimum expected cost is $f_0(2) = 590,972.22$.\n",
    "\n",
    "* The optimal policy is as follows:\n",
    "    - Run 1: Produce 2. We will end up with 2, 1, or 0 missiles needed.\n",
    "    - Run 2: If 2 missiles are still needed, produce 2. If 1 missile is still needed, produce 1. Otherwise produce 0. We will end up with 2, 1, or 0 missiles needed.\n",
    "    - Run 3: If 2 missiles are needed, we're screwed; producing 2 or 0 missiles result in the same expected cost. If 1 missile is still needed, produce 1. Otherwise produce 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (Farkas Investments, revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have recently been hired as a junior analyst at Farkas Investments. You have been given \\$4 million to invest over the next 3 years. At the beginning of each of the next 3 years, you can invest in one of two investments: A or B.\n",
    "\n",
    "| Investment | Cost (\\$ millions) | Profit (\\$ millions) | Probability |\n",
    "|:-----------|:-------------------|:---------------------|:------------|\n",
    "| A          | 3                  | 2                    | 0.5         |\n",
    "|            |                    | -2                   | 0.5         |\n",
    "| B          | 5                  | 3                    | 0.1         |\n",
    "|            |                    | -1                   | 0.9         |\n",
    "\n",
    "You are allowed to make at most one investment each year. Any additional money accumulated is left idle. You may not borrow money to invest; that is, you cannot buy into an investment if it costs more than you currently have.\n",
    "\n",
    "Formulate a stochastic dynamic program to find an investment policy that maximizes the probability you will have at least \\$10 million at the end of 3 years.\n",
    "\n",
    "Once upon a time, for homework, you formulated this problem as a stochastic dynamic program.\n",
    "\n",
    "1. Solve your dynamic program using `stochasticdp`.\n",
    "2. Interpret the output of your stochastic dynamic program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-to-go function:\n",
      "{(stage: 0, state: 0): 0\n",
      " (stage: 0, state: 1): 0\n",
      " (stage: 0, state: 2): 0\n",
      " (stage: 0, state: 3): 0.025\n",
      " (stage: 0, state: 4): 0.125\n",
      " (stage: 0, state: 5): 0.125\n",
      " (stage: 0, state: 6): 0.25\n",
      " (stage: 0, state: 7): 0.325\n",
      " (stage: 0, state: 8): 0.625\n",
      " (stage: 0, state: 9): 0.625\n",
      " (stage: 0, state: 10): 1\n",
      " (stage: 1, state: 0): 0\n",
      " (stage: 1, state: 1): 0\n",
      " (stage: 1, state: 2): 0\n",
      " (stage: 1, state: 3): 0.0\n",
      " (stage: 1, state: 4): 0.0\n",
      " (stage: 1, state: 5): 0.05\n",
      " (stage: 1, state: 6): 0.25\n",
      " (stage: 1, state: 7): 0.25\n",
      " (stage: 1, state: 8): 0.5\n",
      " (stage: 1, state: 9): 0.55\n",
      " (stage: 1, state: 10): 1\n",
      " (stage: 2, state: 0): 0\n",
      " (stage: 2, state: 1): 0\n",
      " (stage: 2, state: 2): 0\n",
      " (stage: 2, state: 3): 0.0\n",
      " (stage: 2, state: 4): 0.0\n",
      " (stage: 2, state: 5): 0.0\n",
      " (stage: 2, state: 6): 0.0\n",
      " (stage: 2, state: 7): 0.1\n",
      " (stage: 2, state: 8): 0.5\n",
      " (stage: 2, state: 9): 0.5\n",
      " (stage: 2, state: 10): 1\n",
      " (stage: 3, state: 0): 0\n",
      " (stage: 3, state: 1): 0\n",
      " (stage: 3, state: 2): 0\n",
      " (stage: 3, state: 3): 0\n",
      " (stage: 3, state: 4): 0\n",
      " (stage: 3, state: 5): 0\n",
      " (stage: 3, state: 6): 0\n",
      " (stage: 3, state: 7): 0\n",
      " (stage: 3, state: 8): 0\n",
      " (stage: 3, state: 9): 0\n",
      " (stage: 3, state: 10): 1}\n",
      "\n",
      "Corresponding policy:\n",
      "{(stage: 0, state: 0): {'no investment'}\n",
      " (stage: 0, state: 1): {'no investment'}\n",
      " (stage: 0, state: 2): {'no investment'}\n",
      " (stage: 0, state: 3): {'A'}\n",
      " (stage: 0, state: 4): {'A'}\n",
      " (stage: 0, state: 5): {'A'}\n",
      " (stage: 0, state: 6): {'A', 'no investment'}\n",
      " (stage: 0, state: 7): {'B'}\n",
      " (stage: 0, state: 8): {'A'}\n",
      " (stage: 0, state: 9): {'A'}\n",
      " (stage: 0, state: 10): {'no investment'}\n",
      " (stage: 1, state: 0): {'no investment'}\n",
      " (stage: 1, state: 1): {'no investment'}\n",
      " (stage: 1, state: 2): {'no investment'}\n",
      " (stage: 1, state: 3): {'A', 'no investment'}\n",
      " (stage: 1, state: 4): {'A', 'no investment'}\n",
      " (stage: 1, state: 5): {'B', 'A'}\n",
      " (stage: 1, state: 6): {'A'}\n",
      " (stage: 1, state: 7): {'A'}\n",
      " (stage: 1, state: 8): {'A', 'no investment'}\n",
      " (stage: 1, state: 9): {'B', 'A'}\n",
      " (stage: 1, state: 10): {'no investment'}\n",
      " (stage: 2, state: 0): {'no investment'}\n",
      " (stage: 2, state: 1): {'no investment'}\n",
      " (stage: 2, state: 2): {'no investment'}\n",
      " (stage: 2, state: 3): {'A', 'no investment'}\n",
      " (stage: 2, state: 4): {'A', 'no investment'}\n",
      " (stage: 2, state: 5): {'B', 'A', 'no investment'}\n",
      " (stage: 2, state: 6): {'B', 'A', 'no investment'}\n",
      " (stage: 2, state: 7): {'B'}\n",
      " (stage: 2, state: 8): {'A'}\n",
      " (stage: 2, state: 9): {'A'}\n",
      " (stage: 2, state: 10): {'no investment'}}\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "# Number of stages\n",
    "number_of_stages = 4\n",
    "\n",
    "# List of states\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# List of decisions\n",
    "decisions = ['A', 'B', 'no investment']\n",
    "\n",
    "# Initialize stochastic DP\n",
    "dp = StochasticDP(number_of_stages, states, decisions, minimize=False)\n",
    "\n",
    "# Transition probabilities and contributions\n",
    "for t in range(number_of_stages - 1):\n",
    "    for n in states:\n",
    "        # Investment A\n",
    "        if (n >= 3):\n",
    "            dp.add_transition(stage=t, from_state=n, decision='A', to_state=min(n + 2, 10), probability=0.5, contribution=0)\n",
    "            dp.add_transition(stage=t, from_state=n, decision='A', to_state=n - 2, probability=0.5, contribution=0)\n",
    "        \n",
    "        # Investment B\n",
    "        if (n >= 5):\n",
    "            dp.add_transition(stage=t, from_state=n, decision='B', to_state=min(n + 3, 10), probability=0.1, contribution=0)\n",
    "            dp.add_transition(stage=t, from_state=n, decision='B', to_state=n - 1, probability=0.9, contribution=0)            \n",
    "        \n",
    "        # No investment\n",
    "        dp.add_transition(stage=t, from_state=n, decision='no investment', to_state=n, probability=1, contribution=0)            \n",
    "        \n",
    "# Boundary conditions\n",
    "for n in states:\n",
    "    if n == 10:\n",
    "        dp.add_boundary(state=n, value=1)\n",
    "    else:\n",
    "        dp.add_boundary(state=n, value=0)\n",
    "\n",
    "# Solve stochastic DP\n",
    "value, policy = dp.solve()\n",
    "\n",
    "# Examine value-to-go\n",
    "print(\"Value-to-go function:\")\n",
    "print(value)\n",
    "\n",
    "# Examine policy\n",
    "print(\"\\nCorresponding policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your notes here. Double-click to edit._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Solution._\n",
    "\n",
    "* The maximum probability of reaching \\\\$10 million is $f_0(4) = 0.125$.\n",
    "\n",
    "* The optimal policy is as follows:\n",
    "    - Year 1: Invest in A. We will end up with either \\\\$2 or \\\\$6 million.\n",
    "    - Year 2: If we end up with \\\\$2 million, don't invest (in fact, we can't, because we don't have enough money). If we end up with \\\\$6 million, invest in A. We can end up with \\\\$4 million or \\$8 million.\n",
    "    - Year 3: If we end up with \\\\$4 million, we can't reach \\\\$10 million by the end of the year. Since our objective in this problem is purely to maximize the probability we reach \\\\$10 million, either investing in A or not investing result in the same value-to-go (0). If we end up with \\\\$8 million, invest in A again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_metadata": {
   "title": "Lesson 19. Solving Stochastic Dynamic Programs with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
